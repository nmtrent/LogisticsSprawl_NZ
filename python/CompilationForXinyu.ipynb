{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling data for Xinyu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from pandas import DataFrame, read_csv\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareAreasIntraFile(df,Household=False):\n",
    "    if Household:\n",
    "        list2013=df.loc[df['Year']==2013].Area.tolist()\n",
    "        list2018=df.loc[df['Year']==2018].Area.tolist()\n",
    "        if list2013==list2018:\n",
    "            print(\"Lists 2013 and 2018 are exactly identical\")\n",
    "        else:\n",
    "            print(\"Lists 2013 and 2018 are DIFFERENT\")\n",
    "    else:\n",
    "        list2006=df.loc[df['Year']==2006].Area.tolist()\n",
    "        list2013=df.loc[df['Year']==2013].Area.tolist()\n",
    "        list2018=df.loc[df['Year']==2018].Area.tolist() \n",
    "        if list2006==list2013:\n",
    "            print(\"Lists 2006 and 2013 are exactly identical\")\n",
    "        else:\n",
    "            print(\"Lists 2006 and 2013 are DIFFERENT\")\n",
    "        \n",
    "        if list2013==list2018:\n",
    "            print(\"Lists 2013 and 2018 are exactly identical\")\n",
    "        else:\n",
    "            print(\"Lists 2013 and 2018 are DIFFERENT\")\n",
    "            \n",
    "def compareAreasInterFile(df1,df2,year):\n",
    "    list1=df1.loc[df1['Year']==year].Area.tolist()\n",
    "    list2=df2.loc[df2['Year']==year].Area.tolist()\n",
    "    if list1==list2:\n",
    "        print(\"Lists are exactly identical\")\n",
    "    else:\n",
    "        print(\"Lists are DIFFERENT\")\n",
    "        \n",
    "def createGroupARow(area,year,pArea):\n",
    "    dict1={'Area':area, 'TotInd_GeogUnits':np.nan, 'TotInd_EmpCo':np.nan, 'F_GeogUnits':np.nan, 'F_EmpCo':np.nan,\n",
    "       'G_GeogUnits':np.nan, 'G_EmpCo':np.nan, 'I461_GeogUnits':np.nan, 'I461_EmpCo':np.nan,\n",
    "       'I471_GeogUnits':np.nan, 'I471_EmpCo':np.nan, 'I481_GeogUnits':np.nan, 'I481_EmpCo':np.nan,\n",
    "       'I51_GeogUnits':np.nan, 'I51_EmpCo':np.nan, 'I53_GeogUnits':np.nan, 'I53_EmpCo':np.nan,\n",
    "       'ParentArea':pArea, 'Year':year}\n",
    "    return(dict1)\n",
    "\n",
    "def createGroupGRow(area,year,pArea):\n",
    "    dict1={'Area':area, 'TotHousehold':np.nan, 'MedInc':np.nan, 'less20k':np.nan, 'bet20k_30k':np.nan,\n",
    "                  'bet30k_50k':np.nan,'bet50k_70k':np.nan, 'bet70k_100k':np.nan, 'bet100k_150k':np.nan,\n",
    "                  'greater150k':np.nan, 'totStated':np.nan,'totNotStated':np.nan, 'ParentArea':pArea, 'Year':year}\n",
    "    return(dict1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in files and ensure intra-file consistency (SA2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6615, 19)\n",
      "Lists 2006 and 2013 are exactly identical\n",
      "Lists 2013 and 2018 are exactly identical\n"
     ]
    }
   ],
   "source": [
    "# Group A\n",
    "groupA=pd.read_csv(\"CompleteSet_GroupA.csv\")\n",
    "groupA=groupA.drop(\"Unnamed: 0\",axis=1)\n",
    "#Strip all leading whitespace in Area column\n",
    "groupA['Area'] = groupA['Area'].apply(lambda x: x.strip())\n",
    "\n",
    "#Filter only for 2006, 2013 and 2018\n",
    "groupA = groupA.loc[(groupA['Year'] == 2006) | (groupA['Year'] == 2013)| (groupA['Year']==2018)]\n",
    "\n",
    "#Remove total NZ row\n",
    "groupA = groupA.loc[(groupA['Area'] != \"Total NZ by Regional Council/Statistical Area\")]\n",
    "\n",
    "#Remove total regions\n",
    "groupA = groupA.loc[(groupA['ParentArea'] != \"NewZealand\")]\n",
    "\n",
    "print(groupA.shape)\n",
    "\n",
    "compareAreasIntraFile(groupA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6759, 39)\n",
      "Lists 2006 and 2013 are exactly identical\n",
      "Lists 2013 and 2018 are exactly identical\n"
     ]
    }
   ],
   "source": [
    "# Group D\n",
    "groupD=pd.read_csv(\"CompleteSet_GroupD.csv\")\n",
    "groupD=groupD.drop(\"Unnamed: 0\",axis=1)\n",
    "#Strip all leading whitespace in Area column\n",
    "groupD['Area'] = groupD['Area'].apply(lambda x: x.strip())\n",
    "\n",
    "#Remove total NZ row\n",
    "groupD = groupD.loc[(groupD['Area'] != \"Total - New Zealand by Regional Council/SA2\")]\n",
    "#Remove total regions\n",
    "groupD = groupD.loc[(groupD['ParentArea'] != \"NewZealand\")]\n",
    "print(groupD.shape)\n",
    "\n",
    "compareAreasIntraFile(groupD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6759, 10)\n",
      "Lists 2006 and 2013 are exactly identical\n",
      "Lists 2013 and 2018 are exactly identical\n"
     ]
    }
   ],
   "source": [
    "# Group F\n",
    "groupF=pd.read_csv(\"CompleteSet_GroupF.csv\")\n",
    "groupF=groupF.drop(\"Unnamed: 0\",axis=1)\n",
    "#Strip all leading whitespace in Area column\n",
    "groupF['Area'] = groupF['Area'].apply(lambda x: x.strip())\n",
    "\n",
    "\n",
    "#Remove total NZ row\n",
    "groupF = groupF.loc[(groupF['Area'] != \"Total - New Zealand by Regional Council/SA2\")]\n",
    "#Remove total regions\n",
    "groupF = groupF.loc[(groupF['ParentArea'] != \"NewZealand\")]\n",
    "print(groupF.shape)\n",
    "\n",
    "compareAreasIntraFile(groupF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6759, 14)\n",
      "Lists 2006 and 2013 are exactly identical\n",
      "Lists 2013 and 2018 are exactly identical\n"
     ]
    }
   ],
   "source": [
    "# Group E\n",
    "groupE=pd.read_csv(\"CompleteSet_GroupE.csv\")\n",
    "groupE=groupE.drop(\"Unnamed: 0\",axis=1)\n",
    "#Strip all leading whitespace in Area column\n",
    "groupE['Area'] = groupE['Area'].apply(lambda x: x.strip())\n",
    "\n",
    "#Remove total NZ row\n",
    "groupE = groupE.loc[(groupE['Area'] != \"Total - New Zealand by Regional Council/SA2\")]\n",
    "#Remove total regions\n",
    "groupE = groupE.loc[(groupE['ParentArea'] != \"NewZealand\")]\n",
    "print(groupE.shape)\n",
    "\n",
    "compareAreasIntraFile(groupE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4506, 14)\n",
      "Lists 2013 and 2018 are exactly identical\n"
     ]
    }
   ],
   "source": [
    "# Group G\n",
    "groupG=pd.read_csv(\"CompleteSet_GroupG.csv\")\n",
    "groupG=groupG.drop(\"Unnamed: 0\",axis=1)\n",
    "#Strip all leading whitespace in Area column\n",
    "groupG['Area'] = groupG['Area'].apply(lambda x: x.strip())\n",
    "\n",
    "\n",
    "#Remove total NZ row\n",
    "groupG = groupG.loc[(groupG['Area'] != \"Total - New Zealand by Regional Council/SA2\")]\n",
    "#Remove total regions\n",
    "groupG = groupG.loc[(groupG['ParentArea'] != \"NewZealand\")]\n",
    "\n",
    "print(groupG.shape)\n",
    "compareAreasIntraFile(groupG,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add missing lines to groupA files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lists are exactly identical\n",
      "Lists are exactly identical\n",
      "Lists are exactly identical\n"
     ]
    }
   ],
   "source": [
    "#Because all lists are identical across years for each group, we only need to compare one year from each group\n",
    "\n",
    "#Groups D, E and F are identical\n",
    "compareAreasInterFile(groupD,groupE,2006)\n",
    "compareAreasInterFile(groupD,groupF,2006)\n",
    "\n",
    "compareAreasInterFile(groupG,groupD,2013)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add missing areas to group A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no areas in setA that are not in setD\n",
      "set()\n",
      "There are 48 areas in setD that are not in setA\n"
     ]
    }
   ],
   "source": [
    "setD=set(groupD.loc[groupD['Year']==2013].Area.tolist())\n",
    "setA=set(groupA.loc[groupA['Year']==2013].Area.tolist())\n",
    "\n",
    "print(\"There are no areas in setA that are not in setD\")\n",
    "setDiff_AD=setA.difference(setD)\n",
    "print(setDiff_AD)\n",
    "\n",
    "setDiff_DA=setD.difference(setA)\n",
    "print(\"There are \"+str(len(setDiff_DA))+\" areas in setD that are not in setA\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6615, 19)\n",
      "(6759, 19)\n",
      "Lists 2006 and 2013 are exactly identical\n",
      "Lists 2013 and 2018 are exactly identical\n"
     ]
    }
   ],
   "source": [
    "appendToA=list(setDiff_DA)\n",
    "years=[2006,2013,2018]\n",
    "print(groupA.shape)\n",
    "\n",
    "rows_list=[]\n",
    "for area in appendToA:\n",
    "    for year in years:\n",
    "        parentArea=list(groupD.loc[(groupD['Area']==area) & (groupD['Year']==year)].ParentArea)\n",
    "        rows_list.append(createGroupARow(area,year,parentArea[0]))\n",
    "\n",
    "        \n",
    "additional=pd.DataFrame(rows_list)\n",
    "groupA = pd.concat([groupA, additional])\n",
    "print(groupA.shape)\n",
    "\n",
    "#Test intra consistency\n",
    "compareAreasIntraFile(groupA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now for the geometry files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spelling(df):\n",
    "    df.loc[df['Area']==\"Putararu Rural\",'Area']=\"Putaruru Rural\"\n",
    "    df.loc[df['Area']==\"Putararu\",'Area']=\"Putaruru\"\n",
    "    df.loc[df['Area']==\"Whangaparoa Central\",'Area']=\"Whangaparaoa Central\"\n",
    "    df.loc[df['Area']==\"Oceanic Manawatu-Wanganui Region East\",'Area']=\"Oceanic Manawatu-Whanganui Region East\"\n",
    "    df.loc[df['Area']==\"Oceanic Manawatu-Wanganui Region West\",'Area']=\"Oceanic Manawatu-Whanganui Region West\"\n",
    "    return(df)\n",
    "\n",
    "def dropAreas(df):\n",
    "#This function is necessary when working with the csv geom file, not the shapefile\n",
    "    df = df[df.Area != 'Antipodes Islands']\n",
    "    df = df[df.Area != 'Auckland Islands']\n",
    "    df = df[df.Area != 'Bounty Islands']\n",
    "    df = df[df.Area != 'Campbell Island']\n",
    "    df = df[df.Area != 'Kermadec Islands']\n",
    "    df = df[df.Area != 'New Zealand Economic Zone']\n",
    "    df = df[df.Area != 'Oceanic Antipodes Islands']\n",
    "    df = df[df.Area != 'Oceanic Auckland Islands']\n",
    "    df = df[df.Area != 'Oceanic Bounty Islands']\n",
    "    df = df[df.Area != 'Oceanic Campbell Island']\n",
    "    df = df[df.Area != 'Oceanic Kermadec Islands']\n",
    "    df = df[df.Area != 'Oceanic Oil Rig Southland']\n",
    "    df = df[df.Area != 'Oceanic Oil Rig Taranaki']\n",
    "    df = df[df.Area != 'Oceanic Snares Islands']\n",
    "    df = df[df.Area != 'Ross Dependency']\n",
    "    df = df[df.Area != 'Snares Islands']\n",
    "    return(df)\n",
    "\n",
    "\n",
    "\n",
    "def addAreas(group):\n",
    "    rows_list=[]\n",
    "    if group == \"A\":\n",
    "        for year in [2006,2013,2018]:\n",
    "            dict1={'Area':'Te Kauwhata West', 'TotInd_GeogUnits':np.nan, 'TotInd_EmpCo':np.nan, 'F_GeogUnits':np.nan,\n",
    "                   'F_EmpCo':np.nan,'G_GeogUnits':np.nan, 'G_EmpCo':np.nan, 'I461_GeogUnits':np.nan, 'I461_EmpCo':np.nan,\n",
    "                   'I471_GeogUnits':np.nan, 'I471_EmpCo':np.nan, 'I481_GeogUnits':np.nan, 'I481_EmpCo':np.nan,\n",
    "                   'I51_GeogUnits':np.nan, 'I51_EmpCo':np.nan, 'I53_GeogUnits':np.nan, 'I53_EmpCo':np.nan,\n",
    "                   'ParentArea':'WaikatoRegion', 'Year':year}\n",
    "            rows_list.append(dict1)\n",
    "            dict1={'Area':'Whale Bay', 'TotInd_GeogUnits':np.nan, 'TotInd_EmpCo':np.nan, 'F_GeogUnits':np.nan,\n",
    "                   'F_EmpCo':np.nan,'G_GeogUnits':np.nan, 'G_EmpCo':np.nan, 'I461_GeogUnits':np.nan, 'I461_EmpCo':np.nan,\n",
    "                   'I471_GeogUnits':np.nan, 'I471_EmpCo':np.nan, 'I481_GeogUnits':np.nan, 'I481_EmpCo':np.nan,\n",
    "                   'I51_GeogUnits':np.nan, 'I51_EmpCo':np.nan, 'I53_GeogUnits':np.nan, 'I53_EmpCo':np.nan,\n",
    "                   'ParentArea':'WaikatoRegion', 'Year':year}\n",
    "            rows_list.append(dict1)\n",
    "    elif group == \"D\":\n",
    "        for year in [2006,2013,2018]:\n",
    "            dict1={'Area':'Te Kauwhata West', 'less5k_TotInd':np.nan, 'less5k_Wholesale':np.nan, 'less5k_Retail':np.nan,\n",
    "                   'less5k_TransPostWare':np.nan, 'bet5k10k_TotInd':np.nan, 'bet5k10k_Wholesale':np.nan,'bet5k10k_Retail':np.nan,\n",
    "                   'bet5k10k_TransPostWare':np.nan, 'bet10k20k_TotInd':np.nan,'bet10k20k_Wholesale':np.nan,\n",
    "                   'bet10k20k_Retail':np.nan, 'bet10k20k_TransPostWare':np.nan,'bet20k30k_TotInd':np.nan, 'bet20k30k_Wholesale':np.nan,\n",
    "                   'bet20k30k_Retail':np.nan,'bet20k30k_TransPostWare':np.nan, 'bet30k50k_TotInd':np.nan, 'bet30k50k_Wholesale':np.nan,\n",
    "                   'bet30k50k_Retail':np.nan, 'bet30k50k_TransPostWare':np.nan, 'bet50k70k_TotInd':np.nan,'bet50k70k_Wholesale':np.nan,\n",
    "                   'bet50k70k_Retail':np.nan, 'bet50k70k_TransPostWare':np.nan,'greater70k_TotInd':np.nan, 'greater70k_Wholesale':np.nan,\n",
    "                   'greater70k_Retail':np.nan,'greater70k_TransPostWare':np.nan, 'totStated_TotInd':np.nan, 'totStated_Wholesale':np.nan,\n",
    "                   'totStated_Retail':np.nan, 'totStated_TransPostWare':np.nan, 'notStated_TotInd':np.nan,'notStated_Wholesale':np.nan,\n",
    "                   'notStated_Retail':np.nan, 'notStated_TransPostWare':np.nan,'ParentArea':'WaikatoRegion', 'Year':year}\n",
    "            rows_list.append(dict1)\n",
    "            dict1={'Area':'Whale Bay', 'less5k_TotInd':np.nan, 'less5k_Wholesale':np.nan, 'less5k_Retail':np.nan,\n",
    "                   'less5k_TransPostWare':np.nan, 'bet5k10k_TotInd':np.nan, 'bet5k10k_Wholesale':np.nan,'bet5k10k_Retail':np.nan,\n",
    "                   'bet5k10k_TransPostWare':np.nan, 'bet10k20k_TotInd':np.nan,'bet10k20k_Wholesale':np.nan,\n",
    "                   'bet10k20k_Retail':np.nan, 'bet10k20k_TransPostWare':np.nan,'bet20k30k_TotInd':np.nan, 'bet20k30k_Wholesale':np.nan,\n",
    "                   'bet20k30k_Retail':np.nan,'bet20k30k_TransPostWare':np.nan, 'bet30k50k_TotInd':np.nan, 'bet30k50k_Wholesale':np.nan,\n",
    "                   'bet30k50k_Retail':np.nan, 'bet30k50k_TransPostWare':np.nan, 'bet50k70k_TotInd':np.nan,'bet50k70k_Wholesale':np.nan,\n",
    "                   'bet50k70k_Retail':np.nan, 'bet50k70k_TransPostWare':np.nan,'greater70k_TotInd':np.nan, 'greater70k_Wholesale':np.nan,\n",
    "                   'greater70k_Retail':np.nan,'greater70k_TransPostWare':np.nan, 'totStated_TotInd':np.nan, 'totStated_Wholesale':np.nan,\n",
    "                   'totStated_Retail':np.nan, 'totStated_TransPostWare':np.nan, 'notStated_TotInd':np.nan,'notStated_Wholesale':np.nan,\n",
    "                   'notStated_Retail':np.nan, 'notStated_TransPostWare':np.nan,'ParentArea':'WaikatoRegion', 'Year':year}\n",
    "            rows_list.append(dict1)\n",
    "    elif group==\"E\":\n",
    "        for year in [2006,2013,2018]:\n",
    "            dict1={'Area':'Te Kauwhata West', 'European':np.nan, 'Maori':np.nan, 'Pacific':np.nan, 'Asian':np.nan, 'MidELatinAfr':np.nan,\n",
    "                   'Other':np.nan, 'NZ':np.nan, 'OtherNEC':np.nan, 'TotPeopleStated':np.nan, 'NotElseIncl':np.nan,'TotPeople':np.nan,\n",
    "                   'ParentArea':'WaikatoRegion', 'Year':year}\n",
    "            rows_list.append(dict1)\n",
    "            dict1={'Area':'Whale Bay', 'European':np.nan, 'Maori':np.nan, 'Pacific':np.nan, 'Asian':np.nan, 'MidELatinAfr':np.nan,\n",
    "                   'Other':np.nan, 'NZ':np.nan, 'OtherNEC':np.nan, 'TotPeopleStated':np.nan, 'NotElseIncl':np.nan,'TotPeople':np.nan,\n",
    "                   'ParentArea':'WaikatoRegion', 'Year':year}\n",
    "            rows_list.append(dict1)\n",
    "    elif group == \"F\":\n",
    "        for year in [2006,2013,2018]:\n",
    "            dict1={'Area':'Te Kauwhata West', 'TotPeople':np.nan, 'EmpFull':np.nan, 'EmpPart':np.nan, 'Unemp':np.nan,\n",
    "                   'NotLabForce':np.nan,'TotPeopleStated':np.nan, 'Unidentifiable':np.nan, 'ParentArea':'WaikatoRegion', 'Year':year}\n",
    "            rows_list.append(dict1)\n",
    "            dict1={'Area':'Whale Bay', 'TotPeople':np.nan, 'EmpFull':np.nan, 'EmpPart':np.nan, 'Unemp':np.nan,\n",
    "                   'NotLabForce':np.nan,'TotPeopleStated':np.nan, 'Unidentifiable':np.nan, 'ParentArea':'WaikatoRegion', 'Year':year}\n",
    "            rows_list.append(dict1)\n",
    "    else:\n",
    "        for year in [2013,2018]:\n",
    "            dict1={'Area':'Te Kauwhata West', 'TotHousehold':np.nan, 'MedInc':np.nan, 'less20k':np.nan, 'bet20k_30k':np.nan,\n",
    "                  'bet30k_50k':np.nan,'bet50k_70k':np.nan, 'bet70k_100k':np.nan, 'bet100k_150k':np.nan,\n",
    "                  'greater150k':np.nan, 'totStated':np.nan,'totNotStated':np.nan, 'ParentArea':'WaikatoRegion', 'Year':year}\n",
    "            rows_list.append(dict1)\n",
    "            dict1={'Area':'Whale Bay', 'TotHousehold':np.nan, 'MedInc':np.nan, 'less20k':np.nan, 'bet20k_30k':np.nan,\n",
    "                  'bet30k_50k':np.nan,'bet50k_70k':np.nan, 'bet70k_100k':np.nan, 'bet100k_150k':np.nan,\n",
    "                  'greater150k':np.nan, 'totStated':np.nan,'totNotStated':np.nan, 'ParentArea':'WaikatoRegion', 'Year':year}\n",
    "            rows_list.append(dict1)\n",
    "    return(rows_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when wanting to match with the csv\n",
    "#df_multipoly=pd.read_csv(\"./GISData/statistical-area-2-2020-generalised-LONLAT.csv\")\n",
    "\n",
    "#when wanting to match with the shapefile\n",
    "check_shp=pd.read_csv(\"./GISData/statsnzstatistical-area-2-2020-generalised-SHP/statistical-area-2-2020-tablecsv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "setSHP=set(check_shp.SA22020_V1_00_NAME_ASCII.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix spelling issues in my combo file\n",
    "groupA=spelling(groupA)\n",
    "groupD=spelling(groupD)\n",
    "groupE=spelling(groupE)\n",
    "groupF=spelling(groupF)\n",
    "groupG=spelling(groupG)\n",
    "\n",
    "# Delete areas not in the polygon file - ONLY WHEN MATCHING TO CSV, DO NOT RUN WHEN MATCHING TO SHP\n",
    "# This is ok to do because nearly all are NaN's or very low numbers <6\n",
    "#groupA=dropAreas(groupA)\n",
    "#groupD=dropAreas(groupD)\n",
    "#groupE=dropAreas(groupE)\n",
    "#groupF=dropAreas(groupF)\n",
    "#groupG=dropAreas(groupG)\n",
    "\n",
    "## Add in the two areas that are in the Poly file but not in the stats files\n",
    "\n",
    "additional=pd.DataFrame(addAreas(\"A\"))\n",
    "groupA = pd.concat([groupA, additional])\n",
    "\n",
    "additional=pd.DataFrame(addAreas(\"D\"))\n",
    "groupD = pd.concat([groupD, additional])\n",
    "\n",
    "additional=pd.DataFrame(addAreas(\"E\"))\n",
    "groupE = pd.concat([groupE, additional])\n",
    "\n",
    "additional=pd.DataFrame(addAreas(\"F\"))\n",
    "groupF = pd.concat([groupF, additional])\n",
    "\n",
    "additional=pd.DataFrame(addAreas(\"G\"))\n",
    "groupG = pd.concat([groupG, additional])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setD=set(groupD.loc[groupD['Year']==2013].Area.tolist())\n",
    "\n",
    "setSHP==setD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Then define only the columns we want to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill na's with zeroes\n",
    "groupA=groupA.fillna(0)\n",
    "groupD=groupD.fillna(0)\n",
    "groupE=groupE.fillna(0)\n",
    "groupF=groupF.fillna(0)\n",
    "groupG=groupG.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain only the facility counts, remove employee counts\n",
    "groupA = groupA.drop(['TotInd_EmpCo', 'F_EmpCo',\n",
    "                          'G_EmpCo', 'I461_EmpCo', 'I471_EmpCo', 'I481_EmpCo',\n",
    "                          'I51_EmpCo', 'I53_EmpCo'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine categories so that they match census data\n",
    "groupA['Wholesale_GeogUnits']=groupA['F_GeogUnits']\n",
    "groupA['Retail_GeogUnits']=groupA['G_GeogUnits']\n",
    "groupA['TransPostWare_GeogUnits']=groupA['I461_GeogUnits']+groupA['I471_GeogUnits']+groupA['I481_GeogUnits']+groupA['I51_GeogUnits']+groupA['I53_GeogUnits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the redundant columns\n",
    "groupA=groupA.drop(['F_GeogUnits', 'G_GeogUnits',\n",
    "       'I461_GeogUnits', 'I471_GeogUnits', 'I481_GeogUnits', 'I51_GeogUnits',\n",
    "       'I53_GeogUnits'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total people, percentage Maori & Pacific, perc Asian\n",
    "groupE['perc_Maori_Pacific']=groupE[[\"Maori\", \"Pacific\"]].sum(axis=1)/groupE['TotPeople']\n",
    "groupE['perc_AsiMidELatinAfr']=groupE[[\"Asian\", \"MidELatinAfr\"]].sum(axis=1)/groupE['TotPeople']\n",
    "\n",
    "#fill in nans caused\n",
    "groupE=groupE.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupE=groupE.drop(['European', 'Maori', 'Pacific', 'Asian', 'MidELatinAfr',\n",
    "       'Other', 'NZ', 'OtherNEC', 'TotPeopleStated', 'NotElseIncl'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove redundant columns\n",
    "\n",
    "groupD = groupD.drop([ 'less5k_Wholesale', 'less5k_Retail',\n",
    "                      'less5k_TransPostWare', 'bet5k10k_Wholesale',\n",
    "                      'bet5k10k_Retail', 'bet5k10k_TransPostWare',\n",
    "                      'bet10k20k_Wholesale', 'bet10k20k_Retail', 'bet10k20k_TransPostWare',\n",
    "                      'bet20k30k_Wholesale', 'bet20k30k_Retail',\n",
    "                      'bet20k30k_TransPostWare', 'bet30k50k_Wholesale',\n",
    "                      'bet30k50k_Retail', 'bet30k50k_TransPostWare',\n",
    "                      'bet50k70k_Wholesale', 'bet50k70k_Retail', 'bet50k70k_TransPostWare',\n",
    "                      'greater70k_Wholesale', 'greater70k_Retail',\n",
    "                      'greater70k_TransPostWare', 'totStated_Wholesale',\n",
    "                      'totStated_Retail', 'totStated_TransPostWare',\n",
    "                      'notStated_Wholesale', 'notStated_Retail', 'notStated_TransPostWare'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new columns\n",
    "groupD['TotInd_TotPeople']=groupD['totStated_TotInd']+groupD['notStated_TotInd']\n",
    "#Perc affluent\n",
    "#when using median of medians, 2006 & 2013 is cat 4 and up; 2018 is cat 5 and up\n",
    "groupD.loc[groupD['Year']==2006,'Perc_affl_50']= (groupD.loc[groupD['Year']==2006,'bet20k30k_TotInd']+groupD.loc[groupD['Year']==2006,'bet30k50k_TotInd']+groupD.loc[groupD['Year']==2006,'bet50k70k_TotInd']+groupD.loc[groupD['Year']==2006,'greater70k_TotInd'])/groupD.loc[groupD['Year']==2006,'TotInd_TotPeople']\n",
    "groupD.loc[groupD['Year']==2013,'Perc_affl_50']= (groupD.loc[groupD['Year']==2013,'bet20k30k_TotInd']+groupD.loc[groupD['Year']==2013,'bet30k50k_TotInd']+groupD.loc[groupD['Year']==2013,'bet50k70k_TotInd']+groupD.loc[groupD['Year']==2013,'greater70k_TotInd'])/groupD.loc[groupD['Year']==2013,'TotInd_TotPeople']\n",
    "groupD.loc[groupD['Year']==2018,'Perc_affl_50']= (groupD.loc[groupD['Year']==2018,'bet30k50k_TotInd']+groupD.loc[groupD['Year']==2018,'bet50k70k_TotInd']+groupD.loc[groupD['Year']==2018,'greater70k_TotInd'])/groupD.loc[groupD['Year']==2018,'TotInd_TotPeople']\n",
    "\n",
    "#when using 75th percentile of medians, 2006 & 2013 is cat 5 and up; 2018 is cat 6 and up\n",
    "groupD.loc[groupD['Year']==2006,'Perc_affl_75']= (groupD.loc[groupD['Year']==2006,'bet30k50k_TotInd']+groupD.loc[groupD['Year']==2006,'bet50k70k_TotInd']+groupD.loc[groupD['Year']==2006,'greater70k_TotInd'])/groupD.loc[groupD['Year']==2006,'TotInd_TotPeople']\n",
    "groupD.loc[groupD['Year']==2013,'Perc_affl_75']= (groupD.loc[groupD['Year']==2013,'bet30k50k_TotInd']+groupD.loc[groupD['Year']==2013,'bet50k70k_TotInd']+groupD.loc[groupD['Year']==2013,'greater70k_TotInd'])/groupD.loc[groupD['Year']==2013,'TotInd_TotPeople']\n",
    "groupD.loc[groupD['Year']==2018,'Perc_affl_75']= (groupD.loc[groupD['Year']==2018,'bet50k70k_TotInd']+groupD.loc[groupD['Year']==2018,'greater70k_TotInd'])/groupD.loc[groupD['Year']==2018,'TotInd_TotPeople']\n",
    "groupD=groupD.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain only the Perc_Affluent columns\n",
    "groupD=groupD.drop(['less5k_TotInd', 'bet5k10k_TotInd', 'bet10k20k_TotInd',\n",
    "       'bet20k30k_TotInd', 'bet30k50k_TotInd', 'bet50k70k_TotInd',\n",
    "       'greater70k_TotInd', 'totStated_TotInd', 'notStated_TotInd','TotInd_TotPeople'],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# totPeople, totLabForce, % of lab force Unemp\n",
    "\n",
    "#calc total labour force\n",
    "groupF['TotLabForce']=groupF['TotPeople']-groupF['NotLabForce']\n",
    "\n",
    "#unemployed as perc of labforce\n",
    "groupF['percUnempLabForce']=groupF['Unemp']/groupF['TotLabForce']\n",
    "\n",
    "#fill in nans caused\n",
    "groupF=groupF.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove redundant columns\n",
    "groupF=groupF.drop(['TotPeople','EmpFull', 'EmpPart', 'Unemp', 'NotLabForce',\n",
    "       'TotPeopleStated', 'Unidentifiable'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain TotHousehold, MedInc, percless\n",
    "groupG['MedInc_house']=groupG['MedInc']\n",
    "groupG['perc_less20k_house']=groupG['less20k']/groupG['TotHousehold']\n",
    "groupG['perc_bet20k30k_house']=groupG['bet20k_30k']/groupG['TotHousehold']\n",
    "groupG['perc_bet30k50k_house']=groupG['bet30k_50k']/groupG['TotHousehold']\n",
    "groupG['perc_bet50k70k_house']=groupG['bet50k_70k']/groupG['TotHousehold']\n",
    "groupG['perc_bet70k100k_house']=groupG['bet70k_100k']/groupG['TotHousehold']\n",
    "groupG['perc_bet100k150k_house']=groupG['bet100k_150k']/groupG['TotHousehold']\n",
    "groupG['perc_greater150k_house']=groupG['greater150k']/groupG['TotHousehold']\n",
    "\n",
    "#fill in nans caused\n",
    "groupG=groupG.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove redundant columns\n",
    "groupG=groupG.drop(['MedInc','less20k', 'bet20k_30k', 'bet30k_50k',\n",
    "       'bet50k_70k', 'bet70k_100k', 'bet100k_150k', 'greater150k', 'totStated',\n",
    "       'totNotStated'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now join the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change order of columns in A\n",
    "cols=['Area','ParentArea','Year','TotInd_GeogUnits',\n",
    "      'Wholesale_GeogUnits','Retail_GeogUnits','TransPostWare_GeogUnits']\n",
    "groupA=groupA[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6765, 7)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6765, 5)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupD.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6765, 6)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6765, 5)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4510, 12)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupG.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4510.0"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6765/3*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "comboFrame= pd.merge(groupA, groupD, how=\"outer\", on=[\"Area\", \"ParentArea\",\"Year\"])\n",
    "comboFrame= pd.merge(comboFrame, groupE, how=\"outer\", on=[\"Area\", \"ParentArea\",\"Year\"])\n",
    "comboFrame= pd.merge(comboFrame, groupF, how=\"outer\", on=[\"Area\", \"ParentArea\",\"Year\"])\n",
    "comboFrame.to_csv(\"NZLogSprawl_DataCombo_noHousehold.csv\")\n",
    "#We no longer use the household data\n",
    "#comboFrame= pd.merge(comboFrame, groupG, how=\"outer\", on=[\"Area\", \"ParentArea\",\"Year\"])\n",
    "#comboFrame.to_csv(\"NZLogSprawl_DataCombo.csv\")\n",
    "#groupG.to_csv(\"NZLogSprawl_Household.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Area',\n",
       " 'ParentArea',\n",
       " 'Year',\n",
       " 'TotInd_GeogUnits',\n",
       " 'Wholesale_GeogUnits',\n",
       " 'Retail_GeogUnits',\n",
       " 'TransPostWare_GeogUnits',\n",
       " 'Perc_affl_50',\n",
       " 'Perc_affl_75',\n",
       " 'TotPeople',\n",
       " 'perc_Maori_Pacific',\n",
       " 'perc_AsiMidELatinAfr',\n",
       " 'TotLabForce',\n",
       " 'percUnempLabForce']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comboFrame.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
